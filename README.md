# ğŸ¡ Project 02 â€” Real Estate Transactions POC (Laplace Immo)

This project is part of my Data Engineer training and simulates a real mission inside **Laplace Immo**, a national network of real estate agencies.  
The company wants to leverage data to gain competitive advantage by analyzing **property transaction prices in France**.

The goal of this project is to build a **Proof of Concept (POC)** demonstrating the feasibility of:

- collecting and structuring national real estate transaction data,  
- designing a normalized relational database,  
- analyzing the housing market with SQL queries,  
- documenting the dataset through a complete **data dictionary**,  
- and presenting results to decision-makers through a slide deck.

---

## ğŸ“˜ Project Context

Laplace Immo wants to understand:

- the evolution of the price per square meter in France,  
- which regions or departments have the most dynamic market,  
- and global trends in real estate transactions.

To achieve this, the company requests a first functional version (POC) of a **structured database** built from open real estate transaction data.

The project is divided into **3 phases**, leading to two main deliverables:

### âœ” Deliverable 1 â€” **Data Dictionary (Spreadsheet)**  
Includes field definitions, formats, constraints, and descriptions.

### âœ” Deliverable 2 â€” **Presentation (PowerPoint / Google Slides)**  
Contains:
- project context,  
- data transformation steps,  
- an extract of the data dictionary,  
- the normalized relational schema,  
- screenshot of the database with populated tables,  
- SQL queries answering business needs,  
- the results of these queries.

---

## ğŸ›  Tools & Technologies Used

- **SQL (PostgreSQL recommended)**  
- **Relational Modeling (ERD / UML)**  
- **Spreadsheet tools** for the data dictionary  
- **PowerPoint / Google Slides** for the presentation  
- **Data transformation techniques**  
- **Normalization (1NF â†’ 3NF)**  

---

## ğŸ“‚ Repository Structure

```
project-02-laplace-immo-real-estate/
â”‚
â”œâ”€â”€ project_files/
â”‚   â””â”€â”€ raw/                  # Raw transaction data and instructions
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ queries.sql           # Queries answering business needs
â”‚
â”œâ”€â”€ diagrams/
â”‚   â””â”€â”€ relational_schema.draw.io  # Normalized ER diagram
â”‚
â”œâ”€â”€ db_tables/
â”‚   â”œâ”€â”€ data_dictionary.xlsx  # Completed data dictionary
â”‚   â””â”€â”€ tables.csvs           # All tables of the database
â”‚
â””â”€â”€ README.md
```

---

## ğŸ§± Database Modeling

The relational schema was designed by:

1. Analyzing the structure of real estate transactions  
2. Identifying entities (e.g., property, transaction, location)  
3. Defining relationships and cardinalities  
4. Applying **normalization** to avoid redundancy and ensure consistency  
5. Creating the SQL schema with **constraints**:
   - Primary keys  
   - Foreign keys  
   - Normalized attributes  
   - Adequate data types  

The final database supports clean querying and scalable enrichment.

---

## ğŸ” SQL Queries Answering Business Needs

The agency requested insights such as:

- What is the **average price per square meter** per region?  
- Which **departments** show the highest number of transactions?  
- What is the **median property price** per city?  
- Which areas show the fastest year-over-year price growth?  

These queries were implemented in `queries.sql` and demonstrated in the final presentation.

---

## ğŸ§¾ Deliverables Summary

### âœ” **Data Dictionary**  
Includes definitions for:
- each column,  
- allowable values,  
- constraints,  
- source details,  
- transformations applied.

### âœ” **Presentation File**
Contains:
- Context & objectives  
- Data cleaning & transformation pipeline  
- Normalized data model  
- Screenshots of the database with loaded tables  
- SQL queries + results  

---

## â–¶ï¸ How to Reproduce the Project

1. Clone the repository  
2. Create a PostgreSQL database  
3. Consult the DB schema in the `diagrams/` folder  
4. Insert the tables from `db_tables/`  
5. Run queries from `sql/queries.sql` to reproduce insights  

---
